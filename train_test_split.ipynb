{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a1acb97-b2b3-4d2d-98b2-7bb346c4ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from collections import defaultdict\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Add bucket names \n",
    "BUCKET_NAME = \"lemondataset\"                 # Your S3 bucket name\n",
    "JSON_KEY = \"lemon-dataset/lemon-dataset/annotations/instances_default.json\"    \n",
    "IMAGE_PREFIX = \"lemon-dataset/lemon-dataset/images/\"                      \n",
    "TRAIN_PREFIX = \"lemon-dataset/lemon-dataset/train/\"                     \n",
    "VAL_PREFIX   = \"lemon-dataset/lemon-dataset/validation/\"                  \n",
    "TEST_PREFIX  = \"lemon-dataset/lemon-dataset/test/\"                       \n",
    "\n",
    "\n",
    "TEST_RATIO = 0.1\n",
    "VAL_RATIO  = 0.1\n",
    "\n",
    "annot_json = download_json_from_s3(BUCKET_NAME, JSON_KEY)\n",
    "with open(annot_json, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "images = data[\"images\"]        \n",
    "annotations = data[\"annotations\"]\n",
    "\n",
    "img_to_cats = {}\n",
    "\n",
    "for ann in annotations:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cat_id = ann[\"category_id\"]\n",
    "\n",
    "    if img_id not in img_to_cats:\n",
    "        img_to_cats[img_id] = []\n",
    "\n",
    "    if cat_id not in img_to_cats[img_id]:\n",
    "        img_to_cats[img_id].append(cat_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0dad4d1-ebca-4dfa-a7da-d3182e401e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions I got from GPT \n",
    "\n",
    "def download_json_from_s3(bucket, key, local_path=\"annotations.json\"):\n",
    "    \"\"\"Download the JSON annotation file from S3.\"\"\"\n",
    "    s3.download_file(bucket, key, local_path)\n",
    "    return local_path\n",
    "\n",
    "def copy_s3_object(bucket, source_key, dest_key):\n",
    "    \"\"\"Copy an object within the same bucket.\"\"\"\n",
    "    copy_source = {\"Bucket\": bucket, \"Key\": source_key}\n",
    "    s3.copy_object(Bucket=bucket, CopySource=copy_source, Key=dest_key)\n",
    "\n",
    "def copy_to_folder(image_ids, target_prefix):\n",
    "    for i_id in image_ids:\n",
    "        file_name = id_to_filename[i_id]\n",
    "        source_key = get_source_key(file_name)\n",
    "\n",
    "        dest_file = file_name.split(\"/\")[-1]\n",
    "        dest_key = target_prefix + dest_file\n",
    "        try:\n",
    "            #print(f\"Copied: {BUCKET_NAME}/{source_key}\")\n",
    "            copy_s3_object(BUCKET_NAME, source_key, dest_key)\n",
    "        except ClientError as e:\n",
    "            print(f\"Error copying {file_name} to {target_prefix}: {e}\")\n",
    "\n",
    "# This last one just because I've played around with several ways to split and I don't want to duplicate/ fill up the buckets\n",
    "\n",
    "def delete_all_objects_with_prefix(bucket, prefix):\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "        if \"Contents\" in page:\n",
    "            objects_to_delete = [{'Key': obj['Key']} for obj in page['Contents']]\n",
    "            s3.delete_objects(Bucket=bucket, Delete={'Objects': objects_to_delete})\n",
    "            print(f\"Deleted {len(objects_to_delete)} objects from {prefix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6e8b53b-2682-4ce9-b9a3-fa6dfe0e17df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1000 objects from lemon-dataset/lemon-dataset/train/\n",
      "Deleted 126 objects from lemon-dataset/lemon-dataset/train/\n",
      "Deleted 269 objects from lemon-dataset/lemon-dataset/validation/\n",
      "Deleted 269 objects from lemon-dataset/lemon-dataset/test/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Delete all objects in train, validation, and test folders\n",
    "for prefix in [TRAIN_PREFIX, VAL_PREFIX, TEST_PREFIX]:\n",
    "    delete_all_objects_with_prefix(BUCKET_NAME, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54d6786c-1b3e-4249-a700-39e8c8e3eaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "ID 1: image_quality\n",
      "ID 2: illness\n",
      "ID 3: gangrene\n",
      "ID 4: mould\n",
      "ID 5: blemish\n",
      "ID 6: dark_style_remains\n",
      "ID 7: artifact\n",
      "ID 8: condition\n",
      "ID 9: pedicel\n"
     ]
    }
   ],
   "source": [
    "category_id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in data[\"categories\"]} # list comprehension making a dict\n",
    "print(\"Categories:\")\n",
    "for cat_id, cat_name in category_id_to_name.items():\n",
    "    print(f\"ID {cat_id}: {cat_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8b89e4a-bbb7-4f49-aa08-2b6195342c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 9 : 1245 images\n",
      "Category 5 : 2048 images\n",
      "Category 2 : 1743 images\n",
      "Category 7 : 451 images\n",
      "Category 6 : 467 images\n",
      "Category 3 : 449 images\n",
      "Category 1 : 5 images\n",
      "Category 4 : 264 images\n",
      "Category 8 : 2 images\n"
     ]
    }
   ],
   "source": [
    "cat_to_imgs = {}\n",
    "\n",
    "for ann in annotations:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cat_id = ann[\"category_id\"]\n",
    "\n",
    "    if cat_id not in cat_to_imgs:\n",
    "        cat_to_imgs[cat_id] = []\n",
    "\n",
    "    if img_id not in cat_to_imgs[cat_id]:\n",
    "        cat_to_imgs[cat_id].append(img_id)\n",
    "\n",
    "# Print the total number of unique images per class\n",
    "for cat_id in cat_to_imgs:\n",
    "    img_ids = cat_to_imgs[cat_id]\n",
    "    count = len(img_ids)\n",
    "    print(\"Category\", cat_id, \":\", count, \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38d4c9-f7f1-46ec-915b-8626020700e5",
   "metadata": {},
   "source": [
    "Originally I was going to treat illness, gangrene and mould as bad categories, and make sure to baalnce them in that way\n",
    "but because most images have at least some patches of illness we won't have to worry about balancing that class as a random sample will handle it\n",
    "we'll balance based on gangrene and mould which are more rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5cec326-9b82-4d94-9ecc-de076de9fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_CATEGORIES = [ 3, 4]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5a065cf5-0daa-4ef8-bd15-aab7e1d811bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 269 | Val: 269 | Train: 1126\n",
      "  # bad lemons in train: 563, # good lemons in train: 563\n"
     ]
    }
   ],
   "source": [
    "# note image ids with bad lemons\n",
    "bad_image_ids = []\n",
    "\n",
    "for img in images:\n",
    "    img_id = img[\"id\"]\n",
    "    if img_id in img_to_cats:\n",
    "        for cat in img_to_cats[img_id]:\n",
    "            if cat in BAD_CATEGORIES:\n",
    "                bad_image_ids.append(img_id)\n",
    "                break  # if it has one of the bad categories add it to the list & move on\n",
    "\n",
    "# map image ID to file name to be able to transfer later\n",
    "id_to_filename = {}\n",
    "all_ids = []\n",
    "\n",
    "for img in images:\n",
    "    img_id = img[\"id\"]\n",
    "    id_to_filename[img_id] = img[\"file_name\"]\n",
    "    all_ids.append(img_id)\n",
    "\n",
    "\n",
    "# random shuffle the id's - from the shuffle grab a random sample for test & val\n",
    "random.shuffle(all_ids)\n",
    "\n",
    "n_test = int(TEST_RATIO * len(all_ids))\n",
    "n_val = int(VAL_RATIO * len(all_ids))\n",
    "\n",
    "test_ids = all_ids[:n_test]\n",
    "val_ids = all_ids[n_test : n_test + n_val]\n",
    "remain_ids = all_ids[n_test + n_val:]\n",
    "\n",
    "# of the remaning select only bad lemons\n",
    "train_bad = []\n",
    "\n",
    "for img_id in remain_ids:\n",
    "    if img_id in bad_image_ids:\n",
    "        train_bad.append(img_id)\n",
    "\n",
    "# Remove those from remaining so we don't use them again\n",
    "remain_ids = [img_id for img_id in remain_ids if img_id not in train_bad]\n",
    "\n",
    "#grab however many good to balance the bad in training\n",
    "bad_count = len(train_bad)\n",
    "remain_good = [img_id for img_id in remain_ids if img_id not in bad_image_ids]\n",
    "\n",
    "# Randomly sample good images, but only up to the number of bad ones\n",
    "train_good = random.sample(remain_good, min(bad_count, len(remain_good)))\n",
    "train_ids = train_bad + train_good\n",
    "\n",
    "print(f\"Test: {len(test_ids)} | Val: {len(val_ids)} | Train: {len(train_ids)}\")\n",
    "print(f\"  # bad lemons in train: {len(train_bad)}, # good lemons in train: {len(train_good)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9a2a545-18dc-4c7b-8fa1-9aaa71b7cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## had to add this because the json had the images included in the file name which was messsing with the bucket prefixes\n",
    "# gpt helped me with this\n",
    "def get_source_key(file_name):\n",
    "\n",
    "    if file_name.startswith(\"images/\"):\n",
    "        file_name = file_name[len(\"images/\"):]\n",
    "    return IMAGE_PREFIX + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1fc8cfe-c148-4450-b4cb-35ea4b989594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying test images...\n",
      "Copying validation images...\n",
      "Copying train images (bad + good)...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Copying test images...\")\n",
    "copy_to_folder(test_ids, TEST_PREFIX)\n",
    "\n",
    "print(\"Copying validation images...\")\n",
    "copy_to_folder(val_ids, VAL_PREFIX)\n",
    "\n",
    "print(\"Copying train images ...\")\n",
    "copy_to_folder(train_ids, TRAIN_PREFIX)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
